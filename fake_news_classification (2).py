# -*- coding: utf-8 -*-
"""Fake News Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12hBhqKWvPqD0WwH0xk3R7xYYlPw1VtwI

# Problem Statement :
"""


"""# Fake News Classification with The Help Of Natural Language Processing Technique.
Fake news detection is a hot topic in the field of natural language processing.
We consume news through several mediums throughout the day in our daily routine, but sometimes it becomes difficult to decide which one is fake and which one is authentic. Our job is to create a model which predicts whether a given news is real or fake.

Project Flow:
    1. Problem Statement
    2. Data Gathering
    3. Data Preprocessing : Here we perform some operation on data
        A. Tokenization
        B. Lower Case
        C. Stopwords
        D. Lemmatization / Stemming
    4. Vectorization (Convert Text data into the Vector):
        A. Bag Of Words (CountVectorizer)
        B. TF-IDF
    5. Model Building :
        A. Model Object Initialization
        B. Train and Test Model
    6. Model Evaluation :
        A. Accuracy Score
        B. Confusition Matrix
        C. Classification Report
    7. Model Deployment
    8. Prediction on Client Data

# Required Libraries
"""

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""# 1. Data Gathering"""

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/News_dataset.csv")
df.head()

"""# 2. Data Analysis"""

df.info()

df['label'].value_counts()

df.shape

df.isna().sum()

df = df.dropna() #Handled Missing values by droping those rows

df.isna().sum()

df.shape

df.reset_index(inplace=True)
df.head()

df['title'][0]

df = df.drop(['id','text','author'],axis = 1)
df.head()

"""# 3. Data Preprocessing

## 1.Tokenization
"""

sample_data = 'The quick brown fox jumps over the lazy dog'
sample_data = sample_data.split()
sample_data

# sample_data2 = 'This is a sentence.'
# sample_data2 = sample_data.split(" ")
# sample_data2

import nltk
nltk.download('punkt')
import nltk

sentence = "This isn't a sentence."

tokens = nltk.word_tokenize(sentence)

print(tokens)

"""## 2. Make Lowercase"""

sample_data = [data.lower() for data in sample_data]
sample_data

"""## 3. Remove Stopwords"""

import nltk
nltk.download('stopwords')

stopwords = stopwords.words('english')
print(stopwords[0:10])
print(len(stopwords))

sample_data = [data for data in sample_data if data not in stopwords]
print(sample_data)
len(sample_data)

"""## 4. Stemming"""

ps = PorterStemmer()
sample_data_stemming = [ps.stem(data) for data in sample_data]
print(sample_data_stemming)

"""## 5. Lemmatization"""

nltk.download("wordnet")

lm = WordNetLemmatizer()
sample_data_lemma = [lm.lemmatize(data) for data in sample_data]
print(sample_data_lemma)

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')

lm = WordNetLemmatizer()
corpus = []

for i in range(len(df)):
    review = re.sub(r'[^a-zA-Z0-9]', ' ', df['title'][i])  # Use r before the regex pattern.
    review = review.lower()
    review = nltk.word_tokenize(review)  # Use nltk.word_tokenize() for tokenization.
    review = [lm.lemmatize(x) for x in review if x not in stopwords.words('english')]
    review = " ".join(review)
    corpus.append(review)

len(corpus)

df['title'][0]

corpus[0]

"""# 4. Vectorization (Convert Text data into the Vector)"""

tf = TfidfVectorizer()
x = tf.fit_transform(corpus).toarray()
x

y = df['label']
y.head()

"""## Data splitting into the train and test"""

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 100, stratify = y )

len(x_train),len(y_train)

len(x_test), len(y_test)

"""# 5. Model Building"""

rf = RandomForestClassifier()
rf.fit(x_train, y_train)

"""# 6. Model Evaluation"""

y_pred = rf.predict(x_test)
accuracy_score_ = accuracy_score(y_test,y_pred)
accuracy_score_

class Evaluation:

    def __init__(self,model,x_train,x_test,y_train,y_test):
        self.model = model
        self.x_train = x_train
        self.x_test = x_test
        self.y_train = y_train
        self.y_test = y_test

    def train_evaluation(self):
        y_pred_train = self.model.predict(self.x_train)

        acc_scr_train = accuracy_score(self.y_train,y_pred_train)
        print("Accuracy Score On Training Data Set :",acc_scr_train)
        print()

        con_mat_train = confusion_matrix(self.y_train,y_pred_train)
        print("Confusion Matrix On Training Data Set :\n",con_mat_train)
        print()

        class_rep_train = classification_report(self.y_train,y_pred_train)
        print("Classification Report On Training Data Set :\n",class_rep_train)


    def test_evaluation(self):
        y_pred_test = self.model.predict(self.x_test)

        acc_scr_test = accuracy_score(self.y_test,y_pred_test)
        print("Accuracy Score On Testing Data Set :",acc_scr_test)
        print()

        con_mat_test = confusion_matrix(self.y_test,y_pred_test)
        print("Confusion Matrix On Testing Data Set :\n",con_mat_test)
        print()

        class_rep_test = classification_report(self.y_test,y_pred_test)
        print("Classification Report On Testing Data Set :\n",class_rep_test)

#Checking the accuracy on training dataset

Evaluation(rf,x_train, x_test, y_train, y_test).train_evaluation()

import nltk
nltk.download('punkt')

#Checking the accuracy on testing dataset
Evaluation(rf,x_train, x_test, y_train, y_test).test_evaluation()



"""# Prediction Pipeline"""

class Preprocessing:

    def __init__(self,data):
        self.data = data

    def text_preprocessing_user(self):
        lm = WordNetLemmatizer()
        pred_data = [self.data]
        preprocess_data = []
        for data in pred_data:
            review = re.sub('^a-zA-Z0-9',' ', data)
            review = review.lower()
            review = review.split()
            # review = [lm.lemmatize(x) for x in review if x not in stopwords]
            review = " ".join(review)
            preprocess_data.append(review)
        return preprocess_data

df['title'][1]

data = 'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'
Preprocessing(data).text_preprocessing_user()

class Prediction:

    def __init__(self,pred_data, model):
        self.pred_data = pred_data
        self.model = model

    def prediction_model(self):
        preprocess_data = Preprocessing(self.pred_data).text_preprocessing_user()
        data = tf.transform(preprocess_data)
        prediction = self.model.predict(data)

        if prediction [0] == 0 :
            return "The News Is Fake"

        else:
            return "The News Is Real"

data = 'FLYNN: Hillary Clinton, Big Woman on Campus  Breitbart'
Prediction(data,rf).prediction_model()

df['title'][3]

user_data = 'In Major League Soccer, Argentines Find a Home and Success - The New York Times'
Prediction(user_data,rf).prediction_model()

user_data = 'Massachusetts Cops Wife Busted for Pinning Fake Home-Invasion Robbery on Black Lives Matter'
Prediction(user_data,rf).prediction_model()

!pip install streamlit
import streamlit as st
import joblib

# Load your machine learning model
model = joblib.load('your_model.joblib')

# Define the Streamlit app
st.title('News Classification App')

user_input = st.text_area('Enter a news article:')
if st.button('Classify'):
    prediction = model.predict([user_input])[0]
    st.write('Prediction:', 'Fake' if prediction == 0 else 'Real')
streamlit run your_app.py
